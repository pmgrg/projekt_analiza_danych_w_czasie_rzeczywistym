{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9aa384-d598-4a16-8d31-4383640727a5",
   "metadata": {},
   "source": [
    "Spark will process data in micro-batches which can be defined by triggers. For example, let's say we define a trigger as 1 second, this means Spark will create micro-batches every second and process them accordingly.\n",
    "\n",
    "\n",
    "### Output modes\n",
    "\n",
    "After processing the streaming data, Spark needs to store it somewhere on persistent storage. Spark uses various output modes to store the streaming data.\n",
    "\n",
    "**Append Mode**: In this mode, Spark will output only newly processed rows since the last trigger.\n",
    "\n",
    "**Update Mode**: In this mode, Spark will output only updated rows since the last trigger. If we are not using aggregation on streaming data (meaning previous records can’t be updated) then it will behave similarly to append mode.\n",
    "\n",
    "**Complete Mode**: In this mode, Spark will output all the rows it has processed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8e7ba6-fb0c-4ac8-b7ee-fa65bbeb5e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77868ab-2100-40d3-bfbb-6faf41d988cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"RateSource\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1209f1a-1b0e-4eee-a0c8-872f707765d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcd975-60e4-43cf-8c3f-59132853ec3f",
   "metadata": {},
   "source": [
    "### Create streaming DataFrame\n",
    "Let’s create our first Spark Streaming DataFrame using rate source. Here we have specified the format as rate and specified rowsPerSecond = 1 to generate 1 row for each micro-batch and load the data into initDF streaming DataFrame. Also, we check if the initDF is a streaming DataFrame or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43b8e8f-c4a3-4d3a-9f94-01d8d4575522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"Rate\")\\\n",
    "        .option(\"rowsPerSecond\", 1)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267d51b4-d6e4-46cf-be76-2bbfe6219178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming DataFrame : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Streaming DataFrame : {initDF.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3997a-8b1c-4c63-9cf2-c193f66362a5",
   "metadata": {},
   "source": [
    "### Basic transformation\n",
    "Perform a basic transformation on initDF to generate another column result by just adding 1 to column value :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f26542f-744e-45dc-99b3-2505a44d0773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultDF = initDF.withColumn(\"result\", f.col(\"value\") + f.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541c7b9-fd6d-468a-baae-2e23b2c61ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultDF.writeStream\\\n",
    ".outputMode(\"append\")\\\n",
    ".option(\"truncate\", False)\\\n",
    ".format(\"console\")\\\n",
    ".start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f4d22-5478-4dd1-a8a4-b77cf18011f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
